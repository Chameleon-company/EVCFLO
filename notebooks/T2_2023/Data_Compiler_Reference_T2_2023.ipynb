{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the data reference compiler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include any packages to import below :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT! Specify the root folder (Input) and Initialise the list (Output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the root folder path where the data files are stored\n",
    "root_folder_path = 'C:/Users/jamie/Documents/GitHub/EVCFLO/datasets'\n",
    "\n",
    "# Initialize data source information list\n",
    "all_data_sources = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Traverse Through Subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Traverse Through Subfolders\n",
    "for root, _, files in os.walk(root_folder_path):\n",
    "    folder_name = os.path.basename(root)\n",
    "    folder_data_sources = []\n",
    "\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            folder_data_sources.append({\"filename\": file_path, \"format\": \"CSV\"})\n",
    "        elif filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            folder_data_sources.append({\"filename\": file_path, \"format\": \"JSON\"})\n",
    "        elif filename.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            folder_data_sources.append({\"filename\": file_path, \"format\": \"Excel\"})\n",
    "        # Add more conditions for other data source formats (e.g., \".xls\" for older Excel format, etc.)\n",
    "    \n",
    "    if folder_data_sources:\n",
    "        all_data_sources[folder_name] = folder_data_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:/Users/jamie/Documents/GitHub/EVCFLO/datasets\\archive\\AUS EV Charging Stations Dataset\\MEVdata1.csv: 'utf-8' codec can't decode byte 0x92 in position 11286: invalid start byte\n",
      "Error processing C:/Users/jamie/Documents/GitHub/EVCFLO/datasets\\archive\\EVCS - Consumption Datasets\\Charging Station Info Barcelona.csv: 'utf-8' codec can't decode byte 0xad in position 1021: invalid start byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamie\\AppData\\Local\\Temp\\ipykernel_58312\\3660678878.py:8: DtypeWarning: Columns (20,31,39,46,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:/Users/jamie/Documents/GitHub/EVCFLO/datasets\\archive\\misc\\Turku Finland 2019.csv: 'utf-8' codec can't decode byte 0xe4 in position 320: invalid continuation byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamie\\AppData\\Local\\Temp\\ipykernel_58312\\3660678878.py:8: DtypeWarning: Columns (6,16,20,31,33,36,39,40,41,43,46,52,53,55,57,58,60,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:/Users/jamie/Documents/GitHub/EVCFLO/datasets\\T1_2023\\IEA World\\IEA-EV-data.xlsx: name 'XLRDError' is not defined\n",
      "Error processing C:/Users/jamie/Documents/GitHub/EVCFLO/datasets\\T1_2023\\United_Kingdom\\national-charge-point-registry.csv: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamie\\AppData\\Local\\Temp\\ipykernel_58312\\3660678878.py:8: DtypeWarning: Columns (13,20,31,39,45,46,55,63,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "C:\\Users\\jamie\\AppData\\Local\\Temp\\ipykernel_58312\\3660678878.py:8: DtypeWarning: Columns (0,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n",
      "C:\\Users\\jamie\\AppData\\Local\\Temp\\ipykernel_58312\\3660678878.py:8: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract Information\n",
    "for folder_name, folder_sources in all_data_sources.items():\n",
    "    for source in folder_sources:\n",
    "        filepath = source[\"filename\"]\n",
    "        \n",
    "        try:\n",
    "            if source[\"format\"] == \"CSV\":\n",
    "                df = pd.read_csv(filepath)\n",
    "                source[\"variables\"] = list(df.columns)\n",
    "                source[\"variable_types\"] = df.dtypes.to_dict()\n",
    "                \n",
    "            elif source[\"format\"] == \"JSON\":\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    # Extract other information from JSON data\n",
    "                    \n",
    "            elif source[\"format\"] == \"Excel\":\n",
    "                try:\n",
    "                    df = pd.read_excel(filepath)\n",
    "                except (UnicodeDecodeError, XLRDError):\n",
    "                    # Skip the file if there's an encoding issue or other error\n",
    "                    continue\n",
    "                \n",
    "                source[\"variables\"] = list(df.columns)\n",
    "                source[\"variable_types\"] = df.dtypes.to_dict()\n",
    "                # Additional processing for Excel files\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Print an error message and continue to the next file\n",
    "            print(f\"Error processing {source['filename']}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code I have written code to skip file formats iI was having difficulties with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Reference Document as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML reference document generated and saved as 'reference.html'.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create Reference Document as HTML\n",
    "html_filename = \"reference.html\"\n",
    "with open(html_filename, \"w\") as html_file:\n",
    "    html_file.write(\"<!DOCTYPE html>\\n\")\n",
    "    html_file.write(\"<html>\\n\")\n",
    "    html_file.write(\"<head>\\n\")\n",
    "    html_file.write(\"    <title>Data Source Reference</title>\\n\")\n",
    "    html_file.write(\"</head>\\n\")\n",
    "    html_file.write(\"<body>\\n\")\n",
    "    html_file.write(\"<h1>Data Source Reference</h1>\\n\")\n",
    "    \n",
    "    for folder_name, folder_sources in all_data_sources.items():\n",
    "        html_file.write(f\"<h2>{folder_name}</h2>\\n\")\n",
    "        for source in folder_sources:\n",
    "            html_file.write(f\"<h3>{source['filename']}</h3>\\n\")\n",
    "            html_file.write(f\"<p><strong>Format:</strong> {source['format']}</p>\\n\")\n",
    "            \n",
    "            if \"variables\" in source:\n",
    "                html_file.write(\"<h4>Variables</h4>\\n<ul>\\n\")\n",
    "                for var, var_type in source[\"variable_types\"].items():\n",
    "                    html_file.write(f\"<li>{var}: {var_type}</li>\\n\")\n",
    "                html_file.write(\"</ul>\\n\")\n",
    "    \n",
    "    html_file.write(\"</body>\\n\")\n",
    "    html_file.write(\"</html>\\n\")\n",
    "\n",
    "print(\"HTML reference document generated and saved as 'reference.html'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason I cannot write to another folder because of premissions..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
